{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>DIGHUM101</b></center>\n",
    "<center>5-4: Classification</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "- Build a Logistic regression text classifier\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import os\n",
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "from nltk.corpus import movie_reviews # Note we're importing the dataset here!\n",
    "import pandas as pd\n",
    "import re # regular expressions\n",
    "import seaborn as sns\n",
    "# CV (multiple train/test splitting)\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Tools to create our DTMs\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "# Speed up your machine learning setup\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Mix up our training and test sets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Textual Data\n",
    "\n",
    "How can we translate this simple model of binary classification to text? Let's look at a corpus from `nltk` and build your own classifier using sklearn's machine learning `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our x (reviews) and y (judgements) variables\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "judgements = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a dataframe\n",
    "movies = pd.DataFrame({\"Reviews\" : reviews, \n",
    "                      \"Judgements\" : judgements})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies.shape)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, this is a corpus of IMDB movie reviews. Someone went through and read each review, labeling it as either \"positive\" or \"negative\". The task we have before us is to create a model that can accurately predict whether a never-before-seen review is positive or negative. \n",
    "\n",
    "From the `movie_reviews` object let's take out the reviews and the judgement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a random review and its judgement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The human annotator's review was:\", movies.Judgements[0])\n",
    "print()\n",
    "print(movies.Reviews[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So right now we have a dataframe of movie reviews in the `Reviews` variable and a list of their corresponding judgements in the `Judgements` column. Awesome. What does this sound like to you? Independent and dependent variables? You'd be right!\n",
    "\n",
    "`Reviews` is our x variable. `Judgements` is our y variable. Let's first reassign x and y for simplicity. While we're at it, we're going to set the random_state for our computer. Remember that this makes our result reproducible. We'll also `shuffle` so that we randomize the order of our observations, and when we split the testing and training data it won't be in a biased order. However, start learning about [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) and when you should use it instead of `shuffle`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements), random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't believe me that all we did is reassign and shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0], print(\"Human annotator's review was:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get meaningful independent variables (words) we have to do some processing too (think DTM!). With `sklearn`'s text pipelines, we can quickly build a text classifier in only a few lines of Python: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validated pipepline\n",
    "\n",
    "Remember training/test splitting? Lets do this `cv = 20` times! Read [here](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) for more info on cross-validation.\n",
    "\n",
    "We should reasonably expect this to perform better than a single training/test split (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated model!\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(random_state = 0, penalty = \"l2\", C = 1000))\n",
    "                     ])\n",
    "\n",
    "scores = cross_val_score(text_clf, x, y, cv = 5)\n",
    "\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline tells us three things happened:\n",
    "\n",
    "1. `CountVectorizer`\n",
    "\n",
    "2. `TfidfTransformer`\n",
    "\n",
    "3. `LogisticRegression`\n",
    "\n",
    "Let's walk through this step by step.\n",
    "\n",
    "1. `CountVectorizer` does the same as before. It changes all the texts to quickly normalized words, and then simply counts the frequency of each word occuring in the corpus for each document. The feature array for each document at this point is simply the length of all unique words in a corpus, with the count for the frequency of each. This is the most basic way to provide features for a classifier - a document term matrix!\n",
    "\n",
    "2. Remember that tfidf (term frequency inverse document frequency) is an algorithm that aims to find words that are important to specific documents. It does this by taking the term frequency (tf) for a specific term in a specific document, and multiplying it by the term's inverse document frequency (idf), which is the total number of documents divided by the number of documents that contain the term at least once. `TfidfTransformer` transforms the `CountVectorizer` into a tf-idf representation. \n",
    "\n",
    "A tfidf value is calculated for each term for each document. The feature arrays for a document is now the tfidf values. \n",
    "\n",
    "> Remember! The tfidf matrix is similar to our document term matrix, only now the values have been weighted according to their distribution across documents.\n",
    "\n",
    "The pipeline now sends these tfidf feature arrays to \n",
    "\n",
    "3. `LogisticRegression`, what we learned from notebook 4-3. We add in an l2 penalization parameter because we have many more independent variables from our `dtm` than observations. \n",
    "\n",
    "Check out the [pipeline documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "The code below breaks this down by each step, but combines the `CountVectorizer` and `TfidfTransformer` in the `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard training/test split (no cross validation)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "# get tfidf values\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(x)\n",
    "x_train = tfidf.transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "\n",
    "# build and test logit\n",
    "logit_class = LogisticRegression(random_state = 0, penalty = \"l2\", C = 1000, max_iter=1000)\n",
    "model = logit_class.fit(x_train, y_train)\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Features\n",
    "\n",
    "After we train the model we can then index the tfidf matrix for the words with the most significant coefficients (remember, independent variables!) to get the most helpful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "top25pos = np.argsort(model.coef_[0])[-25:]\n",
    "print(\"Top features for positive reviews:\")\n",
    "print(list(feature_names[j] for j in top25pos))\n",
    "print()\n",
    "print(\"Top features for negative reviews:\")\n",
    "top25neg = np.argsort(model.coef_[0])[:25]\n",
    "print(list(feature_names[j] for j in top25neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "We can also use our model to classify new reviews – all we have to do is extract the tfidf features from the raw text and send them to the model as our features (independent variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_review = \"This was the most awful worst super bad movie ever!\"\n",
    "\n",
    "tfidf_rev = tfidf.transform([new_bad_review])\n",
    "\n",
    "model.predict(tfidf_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_good_review = \"This movie was great, awesome and good!\"\n",
    "\n",
    "tfidf_rev = tfidf.transform([new_good_review])\n",
    "\n",
    "model.predict(tfidf_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have a look at the probabilities that our model is assigning to a new review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confusing_review = \"I hated most things about it but it was okay\"\n",
    "# add \"I guess\" to the end of this sentence to see what happens\n",
    "\n",
    "tfidf_rev = tfidf.transform([new_confusing_review])\n",
    "\n",
    "print(model.classes_)\n",
    "print(model.predict_proba(tfidf_rev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
