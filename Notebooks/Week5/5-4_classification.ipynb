{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>DIGHUM101</b></center>\n",
    "<center>5-4: Classification</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "- Build a Logistic regression text classifier\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import os\n",
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "from nltk.corpus import movie_reviews # Note we're importing the dataset here!\n",
    "import pandas as pd\n",
    "import re # regular expressions\n",
    "import seaborn as sns\n",
    "# CV (multiple train/test splitting)\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Tools to create our DTMs\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "# Speed up your machine learning setup\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Mix up our training and test sets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Textual Data\n",
    "\n",
    "How can we translate this simple model of binary classification to text? Let's look at a corpus from `nltk` and build your own classifier using sklearn's machine learning `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our x (reviews) and y (judgements) variables\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "judgements = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a dataframe\n",
    "movies = pd.DataFrame({\"Reviews\" : reviews, \n",
    "                      \"Judgements\" : judgements})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies.shape)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.Reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, this is a corpus of IMDB movie reviews. Someone went through and read each review, labeling it as either \"positive\" or \"negative\". The task we have before us is to create a model that can accurately predict whether a never-before-seen review is positive or negative. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic recap of getting counts, count vectors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tokens per document, and a flat list of all tokens\n",
    "tokens_list = [m.split() for m in movies.Reviews]\n",
    "tokens_flat = [token for sublist in tokens_list for token in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total tokens?\n",
    "len(tokens_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique tokens (terms)?\n",
    "uniq_tokens = list(set(tokens_flat))\n",
    "len(uniq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'world' in 'dsjksdfjkfsd world jhsdzjds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('world' in 'dsjksdfjkfsd world jhsdzjds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often does a certain word occur?\n",
    "sum([int('world' in d) for d in movies.Reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating your own doc-term matrix\n",
    "vec_doc0 = [tokens_list[0].count(term) for term in uniq_tokens]\n",
    "print(vec_doc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our x and y\n",
    "\n",
    "From the `movie_reviews` object let's take out the reviews and the judgement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a random review and its judgement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The human annotator's review was:\", movies.Judgements[1])\n",
    "print()\n",
    "print(movies.Reviews[1][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So right now we have a dataframe of movie reviews in the `Reviews` variable and a list of their corresponding judgements in the `Judgements` column. Awesome. What does this sound like to you? Independent and dependent variables? You'd be right! `Reviews` is our x variable. `Judgements` is our y variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring TF-IDF scores and classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `.sample` to randomize our movies (which are currently sorted with all negative reviews first, then all positive reviews). We also slice the result to get only the 50 first rows (just to make the visualization a bit easier to inspect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_movies = movies.sample(frac=1,random_state=11)[:50]\n",
    "\n",
    "# sorting again by judgements\n",
    "sub_movies = sub_movies.sort_values(by='Judgements', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many positive and negative reviews we have now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_movies.Judgements.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the TF-IDF vectorizer on our little dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10,stop_words = \"english\")\n",
    "tfidf = vectorizer.fit_transform(sub_movies.Reviews)\n",
    "\n",
    "# turning our sparse into a dense array\n",
    "tfidf = tfidf.toarray()\n",
    "\n",
    "# getting the names of our output features\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above array shows the TF-IDF values for all the words in our vocab, for all our documents. Using matplotlib's `imshow` we can visualize this array. The lighter dots refer to higher tf-idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,50))\n",
    "_ = plt.imshow(tfidf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "_ = plt.imshow(tfidf[:,30:40]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the array for ID of the relevant word\n",
    "tfidf[:, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and get the associated word for this ID\n",
    "feature_names[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2 = plt.imshow(tfidf[:,70:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[:, 74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[74]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence scores\n",
    "\n",
    "The next thing we can do is build a model on our test data, then look at the confidence scores that it will create for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of 1s and 0s instead of \"pos\" and \"neg\", this will be helpful later\n",
    "int_list = []\n",
    "for each in movies.Judgements:\n",
    "    if each in \"pos\":\n",
    "        int_list.append(1)\n",
    "    else:\n",
    "        int_list.append(0)\n",
    "\n",
    "movies['Judgements_int'] = int_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first reassign x and y for simplicity. While we're at it, we're going to set the `random_state` for our computer. Remember that this makes our result reproducible. We'll also `shuffle` so that we randomize the order of our observations, and when we split the testing and training data it won't be in a biased order. However, start learning about [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) and when you should use it instead of `shuffle`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements_int), random_state = 1)\n",
    "\n",
    "# Split the data \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# Vectorize the data\n",
    "vectorizer = TfidfVectorizer(min_df=5,stop_words = \"english\")\n",
    "vectorizer.fit(x)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression\n",
    "logit_class = LogisticRegression(random_state = 0, penalty = \"l2\", C = 1000, max_iter=1000)\n",
    "model = logit_class.fit(x_train, y_train)\n",
    "\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added in an L2 penalization parameter because we have many more independent variables from our `dtm` (i.e., words) than we have observations. For more info about \"solver\" algorithms (that are used to minimize the cost function), see [here](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions/52388406#52388406).\n",
    "\n",
    "Let's show the confidence scores for our test data - the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(x_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force these into 1s and 0s (essentially recreating the classification) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = [int(p[1] > 0.5) for p in model.predict_proba(x_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at a comparison between our predicted y-values and the actual y-values of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validated pipepline\n",
    "\n",
    "For fhe next example, we'll do the same, but this time in a \"pipeline\", which allows us to assemble the several steps we've been going over. Check out the [pipeline documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "\n",
    "We'll also look at k-fold cross-validation: this procedure is a standard method for estimating the performance of a machine learning algorithm on a dataset.\n",
    "\n",
    "Let's start with shuffling the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements), random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `sklearn`'s text pipelines, we can quickly build a text classifier in only a few lines of Python. \n",
    "\n",
    "This pipeline does three things in a row:\n",
    "\n",
    "1. `CountVectorizer`\n",
    "\n",
    "2. `TfidfTransformer`\n",
    "\n",
    "3. `LogisticRegression`\n",
    "\n",
    "Let's walk through them step by step once more.\n",
    "\n",
    "1. `CountVectorizer` does the same as before. It changes all the texts to quickly normalized words, and then simply counts the frequency of each word occuring in the corpus for each document. The feature array for each document at this point is simply the length of all unique words in a corpus, with the count for the frequency of each. This is the most basic way to provide features for a classifier - a document term matrix!\n",
    "\n",
    "2. Remember that tfidf (term frequency inverse document frequency) is an algorithm that aims to find words that are important to specific documents. It does this by taking the term frequency (tf) for a specific term in a specific document, and multiplying it by the term's inverse document frequency (idf), which is the total number of documents divided by the number of documents that contain the term at least once. `TfidfTransformer` transforms the `CountVectorizer` into a tf-idf representation. \n",
    "\n",
    "A tfidf value is calculated for each term for each document. The feature arrays for a document is now the tfidf values. \n",
    "\n",
    "> Remember! The tfidf matrix is similar to our document term matrix, only now the values have been weighted according to their distribution across documents.\n",
    "\n",
    "The pipeline then sends these tfidf feature arrays to \n",
    "\n",
    "3. `LogisticRegression`, what we learned from notebook 4-3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2),min_df=5,stop_words = \"english\")),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(random_state = 0, penalty = \"l2\", C = 1000, max_iter=200))\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember training/test splitting? Lets do this `cv = 20` times! Read [here](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) for more info on cross-validation. For now, we will use this to calculate the variance in the results obtained by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross_\n",
    "scores = cross_val_score(text_clf, x, y, cv = 20)\n",
    "\n",
    "# Print the scores, and the mean of the scores\n",
    "print(scores, np.mean(scores))\n",
    "\n",
    "# Print the standard deviation to see degree of variance in the results obtained by our model\n",
    "print(scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the different train/test splits result in slightly different accuracy scores. However,\n",
    "the variance of these scores is about 3,5%, meaning that our model has a very low variance--i.e., the prediction that we obtained on one test set is not by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Features\n",
    "\n",
    "After we train the model we can index the tfidf matrix for the words with the most significant coefficients (i.e. the ones best fit to predict the outcome class), to get the most helpful features.\n",
    "\n",
    "we can use the `named_steps` method, which takes a dict key, to access methods from each of the steps in our pipeline. This is how we'll get the feature names for the `CountVectorizer` we used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25pos = np.argsort(model.coef_[0])[-25:]\n",
    "print(\"Top features for positive reviews:\")\n",
    "print(list(feature_names[j] for j in top25pos))\n",
    "print()\n",
    "print(\"Top features for negative reviews:\")\n",
    "top25neg = np.argsort(model.coef_[0])[:25]\n",
    "print(list(feature_names[j] for j in top25neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "We can also use our model to classify new reviews – all we have to do is extract the tfidf features from the raw text and send them to the model as our features (independent variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_review = \"This was the greatest amazing good movie ever!\"\n",
    "\n",
    "tfidf_rev = vectorizer.transform([new_bad_review])\n",
    "\n",
    "model.predict(tfidf_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_good_review = \"This movie was bad, awesome and good!\"\n",
    "\n",
    "tfidf_rev = vectorizer.transform([new_good_review])\n",
    "\n",
    "model.predict(tfidf_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have a look at the probabilities that our model is assigning to a new review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confusing_review = \"I hated most things about it but it was okay\"\n",
    "# add \"I guess\" to the end of this sentence to see what happens\n",
    "\n",
    "tfidf_rev = vectorizer.transform([new_confusing_review])\n",
    "\n",
    "print(model.predict_proba(tfidf_rev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
