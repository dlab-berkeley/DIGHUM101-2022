{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>DIGHUM101</b></center>\n",
    "<center>5-3: Word2vec</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "- Create a word embeddings model using word2vec\n",
    "- Use word2vec to find similarities between words\n",
    "- Plot words using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install new libraries if needed - NOTE this notebook assumes you have Gensim v4 or higher\n",
    "# !pip install gensim\n",
    "\n",
    "# In case you need to upgrade\n",
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import gensim # word2vec model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import re # regular expressions\n",
    "import seaborn as sns\n",
    "# Preprocessing\n",
    "import gensim\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "The word2vec family of algorithms use shallow neural networks to produce word embeddings, or ways to represent similar words similarly as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights = pd.read_csv(\"../../Data/human_rights.csv\")\n",
    "print(human_rights.shape)\n",
    "human_rights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, store the documents we want to explore in a separate dataframe with just one column\n",
    "w2v_df = pd.DataFrame({'Processed': human_rights[\"Text_processed\"]})\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the text of each row into a list\n",
    "# We now have a list of lists - one for each document\n",
    "\n",
    "split_rows = [row.split() for row in w2v_df['Processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tokens do we have?\n",
    "tokens_flat = [token for sublist in split_rows for token in sublist]\n",
    "len(tokens_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can speed up training by counting our cores\n",
    "\n",
    "import multiprocessing \n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Check your installed version of Gensim! This notebook assumes you have v4 or higher\n",
    "from importlib.metadata import version\n",
    "version('gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the word2vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(split_rows,\n",
    "                               vector_size=100, # length of vector\n",
    "                               min_count = 2, # words must appear n times to be considered\n",
    "                               workers = cpu_count-1, # set this as your number of CPU cores minus one\n",
    "                               window = 3, # words around the target word that are treated as context\n",
    "                               sg = 1, # 1 = skip-gram, 0 = CBOW\n",
    "                               seed = 1) # model will be initialized and trained in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary - change .index_to_key to .vocab if running gensim <4\n",
    "words = list(model.wv.index_to_key)\n",
    "\n",
    "# Preview\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the vector for a word in the vocabulary\n",
    "\n",
    "model.wv[\"human\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our word embeddings model is going to be far from perfect--we are training it on a very small dataset with \"only\" 65000 tokens. For reference, the original model which Google created in 2012 (trained on part of the Google News dataset) included 100 billion words!\n",
    "\n",
    "Let's have a look at some similarity scores between words we might expect to be related somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the vectors of two words in the vocabulary\n",
    "model.wv.similarity(\"human\", \"rights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(\"human\", \"law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"justice\", \"law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(\"war\", \"humanity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's picking up on words that could be bigrams as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"united\", \"nations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most similar words to a given word\n",
    "human_words = model.wv.most_similar(\"men\", topn=10)\n",
    "print(type(human_words))\n",
    "human_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a dataframe\n",
    "pd.DataFrame(human_words,columns=['Word', \"Similarity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at most similar vectors to an input word. Let's look at what kinds of contexts the word \"women\" appears in in this human rights dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot words with PCA\n",
    "\n",
    "[Principal component analysis](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) and related dimension-reduction algorithms are an excellent way to visualize multivariate data in reduced dimensional space - such as a 2D scatterplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word2vec vocab vectors\n",
    "features = [model.wv[word] for word in model.wv.index_to_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Gensim 4:\n",
    "#features = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of our PCA\n",
    "\n",
    "# Just look at the first two dimensions - the X and Y axes\n",
    "pca = PCA(n_components = 2)\n",
    "pca_out = pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this with the top words (just to keep things slightly uncluttered - visualizing word embeddings is a tricky job!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,14))\n",
    "sns.scatterplot(x=pca_out[:, 0], y=pca_out[:, 1])\n",
    "words = list(model.wv.index_to_key)\n",
    "# Annotate only the top words \n",
    "for i, word in enumerate(words[0:40]):\n",
    "    plt.annotate(word, size = 8, xy = (pca_out[i, 0], pca_out[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources, if you want to go further \n",
    "\n",
    "https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XuxYm2pKjOQ\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "\n",
    "https://www.datacamp.com/community/blog/spacy-cheatsheet\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
